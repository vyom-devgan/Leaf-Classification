{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the training, validation, and test sets.\n",
    "# Includes data augmentation for the training set, and resizing for the validation and test sets.\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),  # Randomly resize and crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image so the shorter side is 256\n",
    "    transforms.CenterCrop(224),  # Crop the image to 224x224 at the center\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the directories and apply the transformations.\n",
    "# Datasets are assumed to be in 'data/train', 'data/val', and 'data/test'.\n",
    "train_data = datasets.ImageFolder('data/train', transform=train_transform)\n",
    "val_data = datasets.ImageFolder('data/val', transform=val_test_transform)\n",
    "test_data = datasets.ImageFolder('data/test', transform=val_test_transform)\n",
    "\n",
    "# Dataloaders for iterating over datasets. Batch size is 32.\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)  # Shuffle for training\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/vyom/DATA/vyom/School/Advanced Neural Networks Deep Learning For Spatial Analysis/pytorch/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/vyom/DATA/vyom/School/Advanced Neural Networks Deep Learning For Spatial Analysis/pytorch/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet18 model and modify the final fully connected layer for binary classification.\n",
    "# Use GPU if available, otherwise fall back to CPU.\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features  # Get the number of input features for the final layer\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # Modify the final layer for 2 classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # Move the model to the appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/vyom/DATA/vyom/School/Advanced Neural Networks Deep Learning For Spatial Analysis/pytorch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function (cross-entropy for classification) and the Adam optimizer.\n",
    "# The learning rate scheduler reduces the learning rate when validation loss plateaus.\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate of 0.001\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, verbose=True)  # Reduce LR on plateau\n",
    "\n",
    "# Initialize TensorBoard writer for logging\n",
    "writer = SummaryWriter('runs/leaf_classification')\n",
    "\n",
    "# Early stopping criteria\n",
    "early_stopping_patience = 4  # Stop training if no improvement for 4 consecutive epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.2292 Acc: 0.9175\n",
      "val Loss: 0.7187 Acc: 0.8963\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9580\n",
      "val Loss: 0.3577 Acc: 0.9333\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.0954 Acc: 0.9664\n",
      "val Loss: 0.0551 Acc: 0.9778\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.9748\n",
      "val Loss: 0.5700 Acc: 0.7630\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1132 Acc: 0.9497\n",
      "val Loss: 0.0745 Acc: 0.9778\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.0830 Acc: 0.9776\n",
      "val Loss: 0.0174 Acc: 0.9852\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1013 Acc: 0.9706\n",
      "val Loss: 0.0498 Acc: 0.9778\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.0686 Acc: 0.9790\n",
      "val Loss: 0.1219 Acc: 0.9481\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0975 Acc: 0.9692\n",
      "val Loss: 0.0820 Acc: 0.9704\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0957 Acc: 0.9622\n",
      "val Loss: 0.0495 Acc: 0.9926\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0926 Acc: 0.9804\n",
      "val Loss: 0.0291 Acc: 0.9926\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0698 Acc: 0.9790\n",
      "val Loss: 0.0437 Acc: 0.9778\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0539 Acc: 0.9846\n",
      "val Loss: 0.0264 Acc: 0.9926\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0596 Acc: 0.9776\n",
      "val Loss: 0.0193 Acc: 0.9926\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "# Implements early stopping, learning rate scheduling, and TensorBoard logging.\n",
    "num_epochs = 25\n",
    "best_model_wts = model.state_dict()\n",
    "best_acc = 0.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "            dataloader = train_loader\n",
    "        else:\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            dataloader = val_loader\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over the data\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize only during training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Track statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "        # Log the metrics to TensorBoard\n",
    "        writer.add_scalar(f'{phase} Loss', epoch_loss, epoch)\n",
    "        writer.add_scalar(f'{phase} Accuracy', epoch_acc, epoch)\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        # Adjust learning rate only on validation phase\n",
    "        if phase == 'val':\n",
    "            scheduler.step(epoch_loss)\n",
    "\n",
    "            # Save the best model based on validation accuracy\n",
    "            if epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "                early_stop_counter = 0  # Reset early stopping counter if accuracy improves\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "    # Early stopping check\n",
    "    if early_stop_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "# Switch to evaluation mode and compute predictions\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store labels and predictions\n",
    "all_labels = []\n",
    "all_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_preds.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics using scikit-learn\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='binary')\n",
    "recall = recall_score(all_labels, all_preds, average='binary')\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print the evaluation metrics for accuracy, precision, recall, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {precision:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3 0]\n",
      " [0 5]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Confusion Matrix:\\n{conf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
